%\documentclass[nojss]{jss}
\documentclass[]{article}
\usepackage{verbatim}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{enumitem}

\usepackage{algorithmicx}
\usepackage{algorithm}% http://ctan.org/pkg/algorithm
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
%\renewcommand{\algorithmicrequire}{\textbf{Input}}
%\renewcommand{\algorithmicensure}{\textbf{Output}}

\usepackage[nounderscore]{syntax}
\setlength{\grammarparsep}{2pt plus 1pt minus 1pt} % increase separation between rules
\setlength{\grammarindent}{6em} % increase separation between LHS/RHS

% Grammar Definitions
\newcommand{\ntelem}[1] {{\textless \textit{#1}\textgreater}}
\newcommand{\ntgram}[1] {{\texttt{<{#1}>}}}

% Rewriting JSS commands
\newcommand{\code}[1] {{\texttt{#1}}}
\newcommand{\pkg}[1]  {{\bf{#1}}}
%\newcommand{\citep}[1] {{(\cite{#1})}}
\newcommand{\citep}[1] {{\cite{#1}}}
\newcommand{\proglang}[1]  {{#1}}

<<echo=FALSE>>=
options(width=70)
options(warn = (-1))
options(prompt="R> ")
@

<<echo=FALSE>>=
set.seed(0)
@

%opening
%\VignetteIndexEntry{Grammatical Evolution: An Informal Tutorial using gramEvol}
\title{Grammatical Evolution: An Informal Tutorial using \pkg{gramEvol}}
\author{Farzad Noorian, Anthony M. de Silva, Philip H.W. Leong}

\begin{document}

\maketitle

%\begin{abstract}
%\end{abstract}

\section{Introduction}

Grammatical evolution (GE) is an evolutionary search algorithm, similar to genetic programming (GP).
It is used to generate programs defined through a grammar.
The original author's website is a good resource for a formal introduction to this technique:
\begin{itemize}
	\item \url{http://www.grammatical-evolution.org/}
\end{itemize}

This document serves as a quick and informal tutorial on how GE works, with examples in \proglang{R}.

\section{Grammatical Evolution}
\subsection{What is a grammar}
A grammar is a way to describe the structure of a sentence in a language. As programs are also considered 
collection of a sentence in a language, grammars are extensively used in computer science for describing them.

A \emph{Context-free grammar} (CFG) is a certain class of grammars in formal language theory.
In CFGs, there are two type of words, refered to as \emph{symbols}: \emph{Terminal} and \emph{Non-terminal}.

Non-terminal symbols are used to describe the class of words.
For example: \ntelem{subject} \ntelem{verb} \ntelem{object}

The style of covering non-terminals in angle brakets (< and >) is referred to as \emph{Backus--Naur Form}, or BNF.

In a CFG, each non-terminal is replaced with another symbol to create a sentence. For example:
\begin{grammar}
	<sentence> ::= \ntelem{subject} \ntelem{verb} \ntelem{object} | \ntelem{subject} \ntelem{verb}
	\hspace*{\fill} (1.a), (1.b)
	
	<subject>  ::= I | You | They
	\hspace*{\fill} (2.a), (2.b), (2.c)
	
	<verb>     ::= read | write | check
	\hspace*{\fill} (3.a), (3.b), (3.c)
	
	<object>   ::= books | stories | academic papers
	\hspace*{\fill} (4.a), (4.b), (4.c)
\end{grammar}
Each line in the above table is called a \emph{production rule}. The ``|" symbols separate different replacement possibilities.
Notice that the first rule, \ntelem{sentence}, is made of other non-terminals.

Also notice that each rule is numbered. Using these numbers, one can describe any valid structure.
This is performed by replacing the first non-terminal symbol with the $n$th rule of that symbol.
For example, 
\begin{itemize}
	\item Starting Symbol: \ntelem{sentence}.
	\item Sequence: 2, 3, 1
\end{itemize}
selects rules (1.b), (2.c) and (3.a) in the following four-step sequence:

%\begin{table}[h]%\centering
\begin{tabular}{ c c c c l }
	\toprule
	Step & Sequence & Rule     &   & Current state                   \\ \midrule
	0    &          & Starting & ~ & \ntelem{sentence}.              \\
	1    & 2        & (1.b)    & ~ & \ntelem{subject} \ntelem{verb}. \\
	2    & 3        & (2.c)    & ~ & They \ntelem{verb}.             \\
	3    & 1        & (3.a)    & ~ & They read.                      \\ \bottomrule
\end{tabular}
%\end{table}

\subsection{Evolutionary optimisation}

Evolutionary optimisation algorithms are a class of optimisation techniques inspired by natural evolution.
They are used in cases where:
\begin{itemize}
	\item The solution to the problem has a certain structure. For example, it is an array of binary variables or integer numbers.
	\item There exist a cost function which can quickly return the \emph{cost} or \emph{fitness} of any candidate solution.
	\item Solving the problem using gradient descent techniques is hard or impossible, because the cost function is non-smooth,
	or has multiple local optimas, or is simply discrete, like the travelling salesman problem (or in hindsight, the grammar).
\end{itemize}

The oldest and simplest of these algorithms is the genetic algorithm (GA), which optimises a vector of binary variables.
In this vignette, when referring to GA, we refer to an extended GA which handles integers numbers.

For an in depth introduction, readers are referred to \href{https://en.wikipedia.org/wiki/Evolutionary_algorithm}{Wikipedia}.

\subsection{Evolving a grammar}

The goal of using GE is to automatically generate a valid expression or multiple expressions, 
in order to create a full program:
\begin{enumerate}
	\item A \emph{grammar} is defined to describe the structure and functions used in the grammar.
	\item A \emph{cost function} is defined to assess the quality of the program.
	\item An \emph{evolutionary algorithm}, like GA, is used to find the sequence that creates the best program.
	\item The sequence is used to generate the program.
\end{enumerate}

\subsection{Applications of grammatical evolution}

As GE is versatile, any application which needs a program, definable by grammar is creatable in GE.
These include computational finance, music, and robot control, along many other uses. 
The authors have previously used GE for electricity forecasting in \cite{MNDL_13}.
See \url{http://www.grammatical-evolution.org/pubs.html} for a collection of publications in this area.

%These following classic problems are usually used to demonstrate the abilities of GE (or other algorithms like GP)
%and are used in this vignette.
%\begin{itemize}
%	\item \href{https://en.wikipedia.org/wiki/Symbolic_regression}{Symbolic regression}
%	\item \href{https://en.wikipedia.org/wiki/Santa_Fe_Trail_problem}{Santa Fe Trail problem}
%\end{itemize}

 
\section{gramEvol Package}

The package \pkg{gramEvol} simplifies defining a grammar and offers a GA implementation. 
The only things the user has to do is defining the grammar and the cost function.
In this vignette, examples are brought to demonstrate this.

\section{Symbolic Regression}
Symbolic regression is the process of discovering a function, in symbolic form,
which fits a given set of data. Evolutionary algorithms such as GP and GE are commonly used in Symbolic Regression problems.
See \href{https://en.wikipedia.org/wiki/Symbolic_regression}{Wikipedia} for an in-depth introduction and some examples.

\subsection{Rediscovery of Kepler's law}
In this section, rediscovery of Kepler's law is presented as an example in symbolic regression.
Rediscovering scientific laws has been subject of study, specially using GP or Gene expression programming (See
\citep{koza1992genetic} or \url{http://www.dtreg.com/gep.htm} for their approach).
%using different heuristic techniques \citep{langley1987heuristics}, GP \citep{koza1992genetic} and Gene expression programming \citep{DTREGGPE}.
Here, the goal is to find a relationship between orbital periods and distances of solar system planets
from the sun. The distance and period data, normalised to Earth, is brought in Table~\ref{tab:planets}.

\begin{table}[!hb]\centering
	\begin{tabular}{ l c r }
		\toprule
		Planet  & Distance & Period \\ \midrule
		Venus   & 0.72     & 0.61   \\
		Earth   & 1.00     & 1.00   \\
		Mars    & 1.52     & 1.84   \\
		Jupiter & 5.20     & 11.90  \\
		Saturn  & 9.53     & 29.40  \\
		Uranus  & 19.10    & 83.50  \\
		\bottomrule
	\end{tabular}
	\caption{Orbit period and distance from the sun for planets in solar system.}
	\label{tab:planets}
\end{table} 

Kepler's third law states:
\begin{equation}
	period^2 = constant \times distance^3
\end{equation}

\subsection{Defining a grammar}
To use grammatical evolution to find this relationship from the data,
we define a grammar as illustrated in Table~\ref{tab:gram_kepler}.
Here $\mathcal{S}$ denotes the starting symbol and $\mathcal{R}$ is the collection of production rules.

\begin{table}[!ht]\centering
	{
		\raggedright
		\hrule
		\vspace{1mm}
		
		%$\mathcal{N}$ = \{$expr$, $sub$-$expr$, $func$, $op$, $var$, $n$\}
		
		%$\mathcal{T}$ = \{\texttt{+}, \texttt{-}, \texttt{$\times$}, \texttt{\^},
		%log, sqrt, sin, cos,
		%\code{distance}, \texttt{1}, \texttt{2}, \texttt{3}, \texttt{4}, \texttt{(}, \texttt{)}\}
		
		$\mathcal{S}$ = \ntelem{expr}
		
		\medskip
		
		Production rules : $\mathcal{R}$
		
		\begin{grammar}
			
			<expr>     ::= <expr><op><expr> | <sub-expr>
			\hspace*{\fill} (1.a), (1.b)
			
			<sub-expr> ::= <func>(<var>) | <var> | <var>$\hat{~}${<n>}
			\hspace*{\fill} (2.a), (2.b), (2.c)
			
			<func>     ::= log | sqrt | sin | cos
			\hspace*{\fill} (3.a), (3.b), (3.c), (3.d)
			
			<op>       ::= + | - | $\times$
			\hspace*{\fill} (4.a), (4.b), (4.c)
			
			<var>      ::= \code{distance} | \code{distance}$\hat{~}$<n> | <n>
			\hspace*{\fill} (5.a), (5.b), (5.c)
			
			<n>        ::= \texttt{1} | \texttt{2} | \texttt{3} | \texttt{4}
			\hspace*{\fill} (6.a), (6.b), (6.c), (6.d)
		\end{grammar}
	}
	\hrule
	\medskip
	\caption{Grammar for discovering Kepler's equation.}
	\label{tab:gram_kepler}
\end{table}
This is a general purpose grammar, and it can create different expressions.

The first step for using \pkg{gramEvol} is loading the grammar
defined in Table~\ref{tab:gram_kepler}:

<<tidy=FALSE>>=
library("gramEvol")

ruleDef <- list(expr  = grule(op(expr, expr), func(expr), var),
                 func  = grule(sin, cos, log, sqrt),
                 op    = grule(`+`, `-`, `*`),
                 var   = grule(distance, distance^n, n),
                 n     = grule(1, 2, 3, 4))

grammarDef <- CreateGrammar(ruleDef)
@
Here,
\begin{itemize}
	\item Rules are defined as list.
	\item Each rule is defined using \code{non.terminal.name = grule(replacement1, replacement2, ...)} format.
	\item \code{CreateGrammar} is used to load the list and create
\end{itemize}
The print function reproduces the grammar in a format similar to Table~\ref{tab:gram_kepler}:
<<>>=
print(grammarDef)
@

Note that \code{`+`} and \code{op(expr, expr)} are used in the code above because \code{grule} expects valid \proglang{R} expressions,
and \code{expr op expr} is not valid. As it might be a nuisance to convert between the functional form and the operator form,
the package also provides \code{gsrule} (or grammar string rule), which accepts string with <>:
<<>>=
ruleDef <- list(expr  = gsrule("<expr><op><expr>", "<func>(<expr>)", "<var>"),
                 func  = gsrule("sin", "cos", "log", "sqrt"),
                 op    = gsrule("+", "-", "*"),
                 var   = grule(distance, distance^n, n),
                 n     = grule(1, 2, 3, 4))

CreateGrammar(ruleDef)
@
Note that \code{gsrule} and \code{grule} are mixed.

\subsection{Defining a cost function}
We use the following equation to normalise the error,
adjusting the impact of error on small values (e.g., Venus)
versus large values (e.g., Uranus):
\begin{equation}
	e = \frac{1}{N} \sum{ log(1 + |p-\hat{p}|)}
\end{equation}
where $e$ is the normalised error,
$N$ is the number of samples,
$p$ is the orbital period and
$\hat{p}$ is the result of symbolical regression.
We implement this as the fitness function \code{SymRegFitFunc}:
<<tidy=FALSE>>=
planets <- c("Venus", "Earth", "Mars", "Jupiter", "Saturn", "Uranus")
distance <- c(0.72, 1.00, 1.52, 5.20, 9.53, 19.10)
period <- c(0.61, 1.00, 1.84, 11.90, 29.40, 83.50)

SymRegFitFunc <- function(expr) {
  result <- eval(expr)

  if (any(is.nan(result)))
    return(Inf)

  return (mean(log(1 + abs(period - result))))
}
@
Notice that a very high cost (infinite error) is assigned 
to invalid expressions in the fitness function.

\subsection{Evolving the grammar}
\code{GrammaticalEvolution} is now ready to run. 
All of the parameters are determined automatically.
To avoid wasting time, and as the best possible outcome and its error are known (because we know the answer),
a \code{terminationCost} is computed and set to terminate GE 
when the Kepler's equation is found.

<<>>=
ge <- GrammaticalEvolution(grammarDef, SymRegFitFunc,  
                           terminationCost = 0.021)
ge
@

Now that the result is found, it can be used in production. Here we only use it in a simple comparison:
<<>>=
best.expression <- ge$best$expression

data.frame(distance, period, Kepler = sqrt(distance^3), 
             GE = eval(best.expression))
@@

%Other GE runs find expressions such as 
%\texttt{sqrt(distance)*distance}
%or \texttt{sqrt(distance\textasciicircum3+cos\\(distance)*log(1\textasciicircum4))}
%$\sqrt{distance}\times distance$,
%$\sqrt{distance^3}$,
%or $\sqrt{distance^3}+\cos(distance)*log(1^4)$
%which all simplify to Kepler's equation.

%The fitness function handles invalid values (e.g., $\log(-1)$)
%by assigning a low fitness (infinite error) to any individual
%with an invalid value. However, \proglang{R} may show warnings
%about \code{NaN}s being produced. To suppress these warnings,
%it is enough to wrap \code{eval} in fitness function
%inside \code{suppressWarnings}:
%\begin{verbatim}
%R> result <- suppressWarnings(eval(expr))
%\end{verbatim}

%As an incomplete search is performed, sometimes the GE
%fails to find a perfect solution. In such cases, a symbolic result
%with error is presented 
%(i.e., $\log(distance)$ in 
%\code{sqrt(distance\textasciicircum3\\+log(distance))}). 
%%$\sqrt{distance^3+\log(distance)}$. 
%To characterise this behaviour,
%the code was run 100 times and its error from Kepler equation was noted.
%The measurements were performed on a single thread on a 3.40 GHz Intel Core i7-2600 CPU.
%To ensure reproducibility, \code{set.seed(0)} was executed before running the code.
%The results are presented in Table~\ref{tab:symreg_performance}.
%Notice that the average performance can be improved at the expense of time,
%by increasing the GE's number of generation \code{iterations} or population size \code{popSize}.

%\begin{table} \centering
%	\begin{tabular}{ l r r r}
%		\toprule
%		Value              & Minimum & Average & Maximum \\ \midrule
%		Error              & 0.00    & 0.92    & 1.61    \\
%		No. of generations & 2.00    & 77.46   & 100.00  \\
%		Time (s)           & 0.40    & 4.16    & 20.31   \\ 
%		Error & 0.00 & 0.00 & 0.00 \\ 
%		No. of generations & 1.00 & 45.24 & 186.00 \\ 
%		Time (s) & 0.08 & 0.79 & 3.17 \\ 
%		\bottomrule
%	\end{tabular}
%	\caption{Summary of grammatical evolution's performance for 100 runs of symbolic regression example.}
%	\label{tab:symreg_performance}
%\end{table} 

\subsection{Monitor function}
A real-world optimisation may take a long time, and a feedback of the state of optimisation is desirable.
\code{GrammaticalEvolution} allows monitoring this status using a callback function.
This function, if provided to parameter \code{monitorFunc}, 
receives an object similar to the return value of
\code{GrammaticalEvolution}.
For example, the following function prints the current generation,
the best individual's expression, its error and produces a plot:

\begin{verbatim}
R> customMonitorFunc <- function(results){
+    cat("-------------------\n")
+    print(results)
+  }

R> ge <- GrammaticalEvolution(grammarDef, SymRegFitFunc, 
+    terminationCost = 0.021,
+    monitorFunc=customMonitorFunc)
\end{verbatim}

or even using the \code{print} function directly:
\begin{verbatim}
R> ge <- GrammaticalEvolution(grammarDef, SymRegFitFunc, 
+    terminationCost = 0.021,
+    monitorFunc = print)
\end{verbatim}

\section{Regular Expression}
\label{sec:reg_expr}

A regular expression (RE) is a sequence of characters that describes a certain character pattern.
Due to their expressiveness, REs are widely used in many string pattern matching tasks, such as searching through log files
or parsing a programs output. See the \href{https://en.wikipedia.org/wiki/Regular_expression}{Wikipedia entry} for an in-depth introduction.

\proglang{R} supports standard regular expression with a Perl-compatible syntax. In addition,
\href{https://github.com/kevinushey/rex}{\pkg{rex} Package} \cite{rex_package} offers a ``Friendly regular expressions for R''.

In this section, we use \pkg{gramEvol} to infer a regular expression from examples by evolving a program made of \pkg{rex} function.
We define the goal as matching a decimal real number, in form of $[\pm]nnn[.nnn]$, where [~] mean optional and $nnn$ denotes numeric characters.
A few good and bad examples are listed below:
<<tidy=FALSE>>=
good <- c("1", "11.1", "1.11", "+11", "-11", "-11.1")
bad <- c("a", "1.", "1..1", "-.1", "-", "1-", "1.-1", 
         ".-1", "1.-", "1.1.1", "", ".", "1.1-", "11-11")
@

Score of any RE is determined by counting the number of good matches and bad rejections:
<<tidy=FALSE>>=
re_score <- function(re) {
  score <- sum(sapply(good, function(x) grepl(re, x))) + 
           sum(sapply(bad, function(x)  !grepl(re, x)))
  return (length(good) + length(bad) - score)
}
@

For GE to be used, a grammar is required:
<<tidy=FALSE>>=
library("rex")

grammarDef <- CreateGrammar(list(
              re    = grule(rex(start, rules, end)),
              rules = grule(rule, .(rule, rules)),
              rule  = grule(numbers, ".", or("+", "-"), maybe(rules))))
grammarDef
@

\begin{itemize}
	\item The first rule, \ntelem{re}, creates a valid \pkg{rex} command 
		  that uses \ntelem{rules} for pattern matching.
	\item The second element, \ntelem{rules}, is recursive and can create collection of rules 
		  such as \ntelem{rule}, \ntelem{rule}, \ntelem{rule}.
		  The \code{.()} allows using a comma inside a grammar, 
		  where otherwise it would have been interpreted as another value in the \code{grule} list.
	\item The last element, \ntelem{rule}, expands to an RE function or character pattern. 
		  These include \code{numbers} (\pkg{rex} shorthand for \code{[:digit:]}),
a decimal point, + or -, and a \code{maybe} function.
A \code{maybe} function allows optional presence of a sequence defined by its own set of \ntelem{rules}.
\end{itemize}

The last step is to perform a search for a regular expression that minimise the score function.
Here the minimum \code{terminationCost} is known, and \code{max.depth} is increased to allow for more expansion of the recursive \ntelem{rules}.
We use \code{GrammaticalExhaustiveSearch} to exhaustively search
for the answer among all possible combinations of the grammar:
% this takes some time to run, use "monitorFunc = print" for more convenience
\begin{verbatim}
R> fitfunc = function(chrom) re_score(eval(chrom))
R> GrammaticalExhaustiveSearch(grammarDef, fitfunc, max.depth=7, terminationCost = 0)

GE Search Results:
Expressions Tested: 6577 
Best Chromosome:    0 1 3 0 2 1 3 1 0 0 1 1 0 0 3 0 0 
Best Expression:    rex(start, maybe(or("+", "-")), maybe(numbers, "."), numbers, maybe(numbers), end) 
Best Cost:          0 
\end{verbatim}

The result, while correct, is different from what we expected: $[\pm][nnn.]nnn[nnn]$, which is true for any real number.

For a similar effort to learn RE using Genetic Programming, see the paper by Bartoli et al.~\cite{bartoli2012automatic}.

\section{Other gramEvol functionality}

In this section, some of the other functionalities of the \pkg{gramEvol} are introduced:

Many examples are demonstrated using the following grammar:
<<tidy=FALSE>>=
grammarDef <- CreateGrammar(list(
                expr = gsrule("(<expr>)<op>(<expr>)", "<coef>*<var>"),
                op   = gsrule("+", "-", "*", "/"),
                coef = gsrule("c1", "c2"),
                var  = gsrule("v1", "v2")))

grammarDef
@

\subsection{Manual mapping}
To \emph{map} a numeric sequence to an expression manually, \code{CreateGrammar} is a useful function:
<<tidy=FALSE>>=
GrammarMap(c(0, 1, 0, 0, 1, 1, 0, 0), grammarDef)
@
The sequence is zero-indexed (the first rule is zero). To see the step by step mapping, use the \code{verbose} option:
<<tidy=FALSE>>=
GrammarMap(c(0, 1, 0, 0, 1, 1, 0, 0), grammarDef, verbose=TRUE)
@

If the length of each gene is insufficient for the genotype to phenotype conversion,
such that a few non-terminal elements still remain in the resulting expression,
a wrapping of up to \code{wrappings} is performed. For example:
<<tidy=FALSE>>=
GrammarMap(c(0, 1, 0, 0, 1, 1), grammarDef, verbose=TRUE)
@


\subsection{Examining a grammar}

\pkg{gramEvol} offers several functions to exmine a defined grammar. 

\code{summary} shows a summary of what grammar presents:
<<tidy=FALSE>>=
summary(grammarDef)
@

Many of these properties are available through individual functions:

\code{GetGrammarDepth} computes the depth of grammar tree.
The parameter \code{max.depth} is used to limit recursion.
For example, this grammar causes infinite recursion because of rule \ntelem{expr} $\rightarrow$ \ntelem{expr}\ntelem{op}\ntelem{expr}. 
By default \code{GetGrammarDepth} limits recursion to the number of symbols defined in the grammar:
<<tidy=FALSE>>=
GetGrammarDepth(grammarDef)
GetGrammarDepth(grammarDef, max.depth = 10)
@

For grammars without recursion, the value returned by 
\code{GetGrammarDepth} is the actual depth of the tree:
<<tidy=FALSE>>=
grammarDef2 <- CreateGrammar(list(
                    expr    = gsrule("(<subexpr>)<op>(<subexpr>)"),
                    subexpr = gsrule("<coef>*<var>"),
                    op      = gsrule("+", "-", "*", "/"),
                    coef    = gsrule("c1", "c2"),
                    var     = gsrule("v1", "v2")))

GetGrammarDepth(grammarDef2)
@

\code{GetGrammarDepth} also supports computing the depth from any symbol:
<<tidy=FALSE>>=
GetGrammarDepth(grammarDef2, startSymb = "<subexpr>")
GetGrammarDepth(grammarDef2, startSymb = "<coef>")
@

\code{GetGrammarMaxRuleSize} returns the maximum number of production rules per symbol.
This function is useful when constructing a GE genome, as it could be used to limit
the codon size and therefore search space. 
For this grammar of \ntelem{op} has the highest number of production rules:
<<tidy=FALSE>>=
GetGrammarMaxRuleSize(grammarDef)
@

\code{GetGrammarNumOfExpressions} returns number of possible expressions existing
  in the grammar space.  
This function also uses parameter \code{max.depth} to limit the number of recursions and 
\code{startSymb} to set the starting symbol:
<<tidy=FALSE>>=
GetGrammarNumOfExpressions(grammarDef)
GetGrammarNumOfExpressions(grammarDef, max.depth = 2)
GetGrammarNumOfExpressions(grammarDef, startSymb = "<coef>")
@
Here, the only expressions with depth of 2 or less
are constructed if rule (\ntelem{coef}$\times$\ntelem{var}) is applied first, creating 4 expressions
(i.e., $c_1 \times v_1$, $c_1 \times v_2$, $c_2 \times v_1$ and $c_2 \times v_2$).
Also if \ntelem{coef} is chosen as the starting symbol, the expressions
are limited to $c_1$ and $c_2$.

\code{GetGrammarMaxSequenceLen} computes the length of integer sequence required
for iterating through the grammar space without wrapping. 
As with the previous functions, 
\code{max.depth} is set to the number of symbols defined in the grammar.
<<tidy=FALSE>>=
GetGrammarMaxSequenceLen(grammarDef)
GetGrammarMaxSequenceLen(grammarDef, max.depth = 3) 
GetGrammarMaxSequenceLen(grammarDef2, startSymb = "<subexpr>")
@

\subsection{Grammatical evolution options}
\code{GrammaticalEvolution} is defined as follows:
\begin{verbatim}
GrammaticalEvolution(grammarDef, evalFunc, 
                     numExpr = 1, 
                     max.depth = GrammarGetDepth(grammarDef),
                     startSymb = GrammarStartSymbol(grammarDef),
                     seqLen = GrammarMaxSequenceLen(grammarDef, max.depth, startSymb),
                     wrappings = 3, 
                     suggestions = NULL,
                     optimizer = c("auto", "es", "ga"),
                     popSize = 8, newPerGen = "auto", elitism = 2,
                     mutationChance = NA,
                     iterations = 1000, terminationCost = NA, 
                     monitorFunc = NULL,
                     plapply = lapply, ...)

\end{verbatim}

\code{max.depth}, \code{startSymb} and \code{seqLen} determine recursive grammar limitations, similar to 
what was explained in the previous section.

The rest of the parameters are the evolutionary optimisation options:
\begin{itemize}
	\item \code{GrammaticalEvolution} evolves a population of \code{popSize} chromosomes for a number of \code{iterations}.
	
	\item if \code{optimizer} is set to ``auto", using the information obtained about the grammar 
		(e.g., number of possibles expressions and maximum sequence length), 
		it automatically determines a suitable value for \code{popSize}, \code{newPerGen} and \code{iterations}.
		
	\item Ordinary cross-over operator is considered destructive when
		homologous production rules are not aligned \citep{oneill2003crossover}, such as in recursive grammars.
		As a result \code{GrammaticalEvolution} uses a heuristic algorithm to 
		improve optimisation results by automatically changing cross-over parameters depending on the grammar.
		A user can turn this off by manually setting the \code{optimizer}.
	
	\item The first generation is made from the \code{suggestions} in form of integer chromosomes,
		  and randomly generated individuals.
	
	\item Each integer chromosome is mapped using the grammar, and its fitness is assessed by calling \code{evalFunc}.

	\item For each generation, the top $n$ scoring chromosomes where $n =$~\code{elitism} are directly
		added to the next generation's population. The rest of the population is created
		using cross-over of chromosomes selected with roulette selection operator.

	\item Each chromosome may mutate by \code{mutationChance}.
		
   	\item After reaching a termination criteria, e.g., the maximum number of \code{iterations} or the desired 
 		\code{terminationCost}, the algorithm stops and returns the best expression found so far.
		
	\item \code{GrammaticalEvolution} also supports multi-gene operations, generating more than one expression
		per chromosome using the \code{numExpr} parameter.
		
	\item \code{monitorFunc} is then called with information and statistics
		about the current status of the population.
		
	\item \code{plapply} is used for parallel processing.

	\item \code{GrammaticalEvolution} automatically filters non-terminal expressions 
		(i.e., expressions that don't yield a terminal expression even after times of \code{wrappings}).
		Therefore the end-user does not need to worry about them while using \pkg{gramEvol}.
\end{itemize}

\subsection{Parallel processing option}

Processing expressions and computing their fitness is often computationally expensive.
The \pkg{gramEvol} package can utilise parallel processing facilities in \proglang{R}
to improve its performance. This is done through the
\code{plapply} argument of \code{GrammaticalEvolution}
function. By default, \code{lapply} function is used to evaluate all individuals in the population.

Multi-core systems simply benefit from using \code{mclapply} from package \pkg{parallel},% \citep{Rlang},
which is a drop-in replacement for \code{lapply} on POSIX compatible systems.  
The following code optimises \code{evalFunc} on 4 cores:

\begin{verbatim}
R> library("parallel")
R> options(mc.cores = 4)
R> ge <- GrammaticalEvolution(grammarDef, evalFunc, 
+                             plapply=mclapply)
\end{verbatim}

To run \pkg{gramEvol} on a cluster,
\code{clusterapply} functions can be used instead. 
The \pkg{gramEvol} package must be first installed on all machines and
the fitness function and its data dependencies exported 
before GE is called. The following
example demonstrates a four-process cluster running on the local machine:

\begin{verbatim}
R> library("parallel")
+  cl <- makeCluster(type = "PSOCK", c("127.0.0.1",
+                                      "127.0.0.1",
+                                      "127.0.0.1",
+                                      "127.0.0.1"))
R> clusterEvalQ(cl, library("gramEvol"))
R> clusterExport(cl, c("evalFunc"))
R> ge <- GrammaticalEvolution(grammarDef, evalFunc, 
+    plapply=function(...) parLapply(cl, ...))
R> stopCluster(cl)
\end{verbatim}

It must be noticed that in any problem
the speed-up achieved depends on the overhead of communication
compared with fitness functions' computational complexity.

\subsection{Group evaluation}

\code{EvalExpressions} offers a simpler interface for evaluating multiple expressions. 
The following example uses a dataset for variables defined in the grammar, and evaluates a GE expression object along 
with a string:
<<tidy=FALSE>>=
df <- data.frame(c1 = c(1,2),
                 c2 = c(2,3),
                 v1 = c(3,4),
                 v2 = c(4,5))

quad.expr <- expression(c1 * v1, c1 * v2, c2 * v1, c2 * v2)
EvalExpressions(quad.expr, envir = df)
@

This function is useful specially with \code{numExpr} parameter of the \code{GrammaticalEvolution}.
It is used in applications when more than one expression is required for an application.
See \cite{MNDL_13} as an example.

\subsection{Alternative optimisation algorithms}

\pkg{gramEvol} also provides a random search and an exhaustive search (which was demonstrated in Section \ref{sec:reg_expr}).
Their syntax is similar to the \code{GrammaticalEvolution}:

\begin{verbatim}
R> result1 <- GrammaticalExhaustiveSearch(grammarDef, evalFunc)
R> result2 <- GrammaticalRandomSearch(grammarDef, evalFunc)
\end{verbatim}


\bibliographystyle{IEEEtran}
\bibliography{vig_refs}  

\end{document}

